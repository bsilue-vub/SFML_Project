{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Research Question 1**\n",
    "#### _\"Consider two classifier models, one being a neural network and the other being a variational neural network. How do these models compare in terms of in-sample and out-of-sample error on a poker dataset?\"_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The research question above addresses two fundamental problems. The first is the comparison between a neural network (NN) and a variational neural network (vNN) on a dataset. The second is the fact that this dataset is a poker dataset, which is a stochastic dataset, meaning that the dataset contains stochastic / noisy behavior from the poker players and that there is class overlap. Indeed, similar game states can still yield different actions.\n",
    "\n",
    "As a result, this work will show what happens when a (v)NN-based classifier must train on a stochastic dataset, and will showcase the difference in performance between these two models.\n",
    "\n",
    "### Literature\n",
    "Neural networks are computational models inspired by the structure and function of the human brain. A neural network consists of interconnected units called neurons that work together to process and learn complex data. The basic structure of a neural network is composed of three main types of layers: input layer, hidden layer(s), and output layer.\n",
    "\n",
    " * The input layer receives the input data and passes it to the next layer.\n",
    " * The hidden layer(s) perform intermediate computations and apply mathematical transformations to the input data.\n",
    " * The output layer produces the final output (i.e. prediction).\n",
    "\n",
    "Indeed, the input data propagates from the input layer, through the hidden layers, to the output layer. This process is known as forward propagation. It involves calculating the weighted sum at each neuron and applying the activation function of the neuron to the result. Indeed, the neurons are interconnected through weighted connections. The weights represent the strengths of the connections and are adjusted during the training process to improve the output. This process is repeated layer by layer until the output layer is reached. An activation function is a non-linear mathematical function applied to the weighted sum of inputs to a neuron. It introduces non-linearity and enables neural networks to model complex relationships in data. In our models, we will use the $Tanh()$ function as the activation function for each neuron.\n",
    "\n",
    "To calculate the performance of the neural network during training such as to be able to improve the weights correctly, we must use a loss function. A loss function measures the error between the predicted output and the true output. For this problem, we used the Mean Squared Error (MSE). The act of actually adjusting the weights based on the loss function during training happens through a process called backpropagation. It involves computing the gradient of the loss function with respect to the network's weights, and then updating the weights using an optimization algorithm such as e.g. gradient descent. This iterative process improves the model's performance over time.\n",
    "\n",
    "Neural networks are very powerful models that can learn complex patterns and make predictions on complex data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "The poker data consists of two lists: \n",
    "* a list of game states with the form [[private cards], [public cards], [bet history]].\n",
    "* a list of actions, for each game state, where c = call, r = raise = bet = b, f = fold, k = check.\n",
    "\n",
    "First, we must pre-process this data such that we work with vectors of bit only. There are 4 rounds: the pre-flop, the flop, the turn, and the river. At the end of all rounds, there have been 7 cards in consideration (2 private cards, 5 public cards). Each card has a rank (there are 13 ranks: 2,3,...K,A), and a suit (there are 4 suits: clubs (♣), diamonds (♦), hearts (♥), and spades (♠)). We convert each rank and each suit to a number, and then convert each number to its binary representation. \n",
    "\n",
    "Since for each of the 7 cards, there are 13 ranks (7 * 4 bits), 4 suits (7 * 3 bits), we get 49 bits to represent the cards. For each round, we have a maximum of 4 raises (4 * 3 bits). In total, we get a state vector of 61 bits. The actions are a one-hot encoding of 4 actions (call, raise, fold, check), so the actions are represented by 4 bits. In total, the state-actions of the poker games are represented by a vector of 65 bits.\n",
    "\n",
    "After converting the data in this format, we will use a pre-trained variational auto-encoder to compress it to a vector of size 46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================#\n",
    "#=== PREPROCESSING INTO VECTOR DATA ===#\n",
    "#======================================#\n",
    "\n",
    "def get_preprocessed_vector_data(preprocessed_readable_data):\n",
    "    \"\"\" Preprocessing of game data into vector state representation.\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing data into vector form.\")\n",
    "    # Get json converters\n",
    "    with open('data/utils/rank2numbr__dataset.json', 'r') as file:\n",
    "        rank2numbr = json.load(file)\n",
    "    with open('data/utils/suit2index__dataset.json', 'r') as file:\n",
    "        suit2index = json.load(file)\n",
    "\n",
    "    count_games = 0\n",
    "    count_states = 0\n",
    "    preprocessed_vector_data = {}\n",
    "\n",
    "    for game_id, game in preprocessed_readable_data.items():\n",
    "        # Pre-process game states\n",
    "        game_states = game[0]\n",
    "        preprocessed_game_vector_states = []\n",
    "\n",
    "        for game_state in game_states:\n",
    "            # Get state information\n",
    "            private_cards = game_state[0]\n",
    "            public_cards = game_state[1]\n",
    "            raises = game_state[2]\n",
    "            # Create vector\n",
    "            cards_vector = get_cards_vector(private_cards, public_cards, \n",
    "                                            rank2numbr, suit2index)\n",
    "            raises_vector = get_raises_vector(raises)\n",
    "            state = np.array(cards_vector + raises_vector)\n",
    "            # Store\n",
    "            preprocessed_game_vector_states.append(state.tolist())\n",
    "            count_states +=1\n",
    "        \n",
    "        # Pre-process game actions\n",
    "        game_actions = game[1]\n",
    "        preprocessed_game_vector_actions = []\n",
    "        action2index = {'c':[1,0,0,0], 'r':[0,1,0,0], 'b':[0,1,0,0], \n",
    "                        'f':[0,0,1,0], 'k':[0,0,0,1]}\n",
    "        for action in game_actions:\n",
    "            action_idx = action2index[action]\n",
    "            preprocessed_game_vector_actions.append(action_idx)\n",
    "        \n",
    "        # Store states and actions\n",
    "        preprocessed_vector_data[game_id] = \\\n",
    "            preprocessed_game_vector_states, preprocessed_game_vector_actions\n",
    "\n",
    "        # Track count\n",
    "        count_games +=1\n",
    "        print(f\"\\rGames processed in vector form: {count_games:,}.\", \n",
    "              end='')\n",
    "\n",
    "    print(\" (states: {:,}).\".format(count_states))\n",
    "    return preprocessed_vector_data\n",
    "\n",
    "def get_cards_vector(private_cards, public_cards, \n",
    "                     rank2numbr, suit2index, card_format='rs'):\n",
    "    cards_vector = []\n",
    "    all_cards = private_cards+public_cards\n",
    "    for card in all_cards:\n",
    "        if card_format=='rs':\n",
    "            rank_binary = number2binarylist(rank2numbr[card[0]], 13)\n",
    "            suit_binary = number2binarylist(suit2index[card[1]], 4)\n",
    "        else:\n",
    "            rank_binary = number2binarylist(rank2numbr[card[1]], 13)\n",
    "            suit_binary = number2binarylist(suit2index[card[0]], 4)\n",
    "        cards_vector += rank_binary+suit_binary\n",
    "    cards_vector += [0]*(49-len(cards_vector))\n",
    "    return cards_vector\n",
    "\n",
    "def get_raises_vector(raises):\n",
    "    raises_vector = []\n",
    "    for r in raises:\n",
    "        binary = number2binarylist(r, 4)\n",
    "        raises_vector += binary\n",
    "    return raises_vector\n",
    "\n",
    "def number2binarylist(number, max_number):\n",
    "    binary_size = len(bin(max_number)[2:])\n",
    "    binary = bin(number)[2:]\n",
    "    binary = '0'*(binary_size-len(binary)) + binary\n",
    "    binary = [int(digit) for digit in binary]\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================#\n",
    "#=== VARIATIONAL AUTO-ENCODER ===#\n",
    "#================================#\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, name, input_dim, hidden_dims, latent_dim, loss_type):\n",
    "        super(VAE, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.name = name\n",
    "\n",
    "        # Dimensions\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(prev_dim, dim))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.fc31 = nn.Linear(prev_dim, latent_dim) # mean\n",
    "        self.fc32 = nn.Linear(prev_dim, latent_dim) # log variance\n",
    "\n",
    "        # Decoder\n",
    "        hidden_dims_reversed = hidden_dims[::-1]\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for dim in hidden_dims_reversed:\n",
    "            decoder_layers.append(nn.Linear(prev_dim, dim))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        decoder_layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, z_mean, z_logvar):\n",
    "        if self.loss_type == \"mse\":\n",
    "            loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        elif self.loss_type == \"bce\":\n",
    "            loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        elif self.loss_type == \"kl\":\n",
    "            kl_loss = -0.5 * \\\n",
    "                torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "            loss = kl_loss\n",
    "        elif self.loss_type == \"wasserstein\":\n",
    "            learned_mean = z_mean.mean(dim=0)\n",
    "            learned_std = torch.exp(0.5 * z_logvar).mean(dim=0)\n",
    "            prior_mean = torch.zeros_like(learned_mean)\n",
    "            prior_std = torch.ones_like(learned_std)\n",
    "            loss = torch.abs(learned_mean - prior_mean).sum() \\\n",
    "                + torch.abs(learned_std - prior_std).sum()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss type specified.\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================#\n",
    "#=== DATA IMPORT ===#\n",
    "#===================#\n",
    "\n",
    "def data_import():\n",
    "    # Import dataset:\n",
    "    print(\"\\nImporting data ...\")\n",
    "    with open('data/poker_data__readable.json') as f:\n",
    "        readable_poker_data = json.load(f)\n",
    "    poker_data_dict = get_preprocessed_vector_data(readable_poker_data)\n",
    "\n",
    "    # Store all states and actions:\n",
    "    print(\"Storing states and actions ...\")\n",
    "    poker_data = []\n",
    "    for game_id, game_data in poker_data_dict.items():\n",
    "        game_states = game_data[0]\n",
    "        game_actions = game_data[1]\n",
    "        for i in range(len(game_states)):\n",
    "            state = game_states[i]\n",
    "            action = game_actions[i]\n",
    "            poker_data.append(state+action)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return poker_data\n",
    "\n",
    "def data_split(split, all_data):\n",
    "    # Create data split:\n",
    "    data_fractions = np.cumsum(split)\n",
    "    split_data = \\\n",
    "        np.split(all_data, (data_fractions[:-1] * len(all_data)).astype(int))\n",
    "    return all_data, split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "poker_data = data_import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================#\n",
    "#=== ENCODE DATA ===#\n",
    "#===================#\n",
    "\n",
    "def encode_data(poker_data):\n",
    "    # VAE setup\n",
    "    vae_name = \"vae\"\n",
    "    vae_input_state_dim = 61\n",
    "    vae_hidden_dims = []\n",
    "    vae_latent_dim = 42\n",
    "    vae_loss_type = \"mse\"\n",
    "    # Instantiate the VAE\n",
    "    vae = VAE(name=vae_name, \n",
    "            input_dim=vae_input_state_dim, \n",
    "            hidden_dims=vae_hidden_dims, \n",
    "            latent_dim=vae_latent_dim, \n",
    "            loss_type=vae_loss_type)\n",
    "    # Import VAE\n",
    "    vae.load_state_dict(torch.load(\"models/vae.pth\"))\n",
    "\n",
    "    encoded_poker_data = []\n",
    "    with torch.no_grad():\n",
    "        for idx, state_action in enumerate(poker_data):\n",
    "            state = state_action[:vae_input_state_dim]\n",
    "            action = state_action[vae_input_state_dim:]\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float)\n",
    "            mu, logvar = vae.encode(state_tensor)\n",
    "            latent = vae.reparameterize(mu, logvar)\n",
    "\n",
    "            encoded_poker_data.append(latent.tolist() + action_tensor.tolist())\n",
    "            print(f\"\\rEncoding data ({idx+1:,}/{len(poker_data):,}).\", end='')\n",
    "    \n",
    "    print(\"\\nDone.\")\n",
    "    return encoded_poker_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode data with VAE\n",
    "encoded_poker_data = encode_data(poker_data=poker_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochasticity\n",
    "Next, we will show that this data is stochastic. The `find_overlaps()` function returns the number of overlaps in the given dataset. We say an overlap occurs when a state has a duplicate somewhere in the dataset, but that state has a different action associated to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================#\n",
    "#=== FIND OVERLAP ===#\n",
    "#====================#\n",
    "\n",
    "def find_overlaps(state_action_tensor, state_dim):\n",
    "    # Definitions\n",
    "    state_tensor = state_action_tensor[:, :state_dim]\n",
    "    \n",
    "    # Get duplicate states\n",
    "    unique_states, inverse_indices, counts = torch.unique(state_tensor, \n",
    "                                                        return_inverse=True, \n",
    "                                                        return_counts=True,\n",
    "                                                        dim=0)\n",
    "    duplicate_states_indices = torch.nonzero(counts > 1).squeeze()\n",
    "    duplicate_states = unique_states[duplicate_states_indices]\n",
    "\n",
    "    # Count ALL state overlaps\n",
    "    overlap_count = 0\n",
    "    for i,state in enumerate(duplicate_states):\n",
    "        duplicate_state_indices = torch.nonzero(torch.all(state_tensor == state, dim=1)).squeeze()\n",
    "        duplicate_state_with_actions = state_action_tensor[duplicate_state_indices, :]\n",
    "\n",
    "        # Check all states that have a duplicate\n",
    "        first_state_action = state_action_tensor[duplicate_state_indices[0]]\n",
    "        equal_state_actions = torch.all(torch.eq(duplicate_state_with_actions, first_state_action), dim=1)\n",
    "        all_equal = torch.all(equal_state_actions)\n",
    "        # Check if one of those duplicates has a different action\n",
    "        if not all_equal:\n",
    "            # If the selected state has duplicates but with different actions, \n",
    "            # that means that this state has a class overlap, so we add them to the count.\n",
    "            overlap_count += len(duplicate_state_indices.tolist())\n",
    "        print(f\"\\rNumber of overlaps found: {overlap_count:,}/{len(state_tensor):,} (Progress: {100*(i+1)/len(duplicate_states):.1f}%).\", end='')\n",
    "\n",
    "    return overlap_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of overlaps in the poker data for similar states\n",
    "state_dim = 61\n",
    "rounded_state_action_tensor = torch.round(torch.tensor(poker_data))\n",
    "overlap_count = find_overlaps(state_action_tensor=rounded_state_action_tensor, \n",
    "                              state_dim=state_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computations above show that nearly 40% of all states have a class overlap. This shows that the dataset is very much stochastic due to the nature of the game of poker, and that the classifiers will not be able to achieve high accuracy. The question is, which model will perform better?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network (NN) classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "Defining the neural network (NN) model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================#\n",
    "#=== NEURAL NETWORK ===#\n",
    "#======================#\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, name, state_dim, hidden_dims, action_dim, \n",
    "                 loss_func, activ_func):\n",
    "        super(NN, self).__init__()\n",
    "        # store input params:\n",
    "        self.name = name\n",
    "        self.input_dim = state_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = action_dim\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        # build generator neural net:\n",
    "        generator_layers = []\n",
    "        prev_dim = state_dim\n",
    "        for dim in hidden_dims:\n",
    "            generator_layers.append(nn.Linear(prev_dim, dim))\n",
    "            generator_layers.append(activ_func)\n",
    "            prev_dim = dim\n",
    "        generator_layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        generator_layers.append(nn.Sigmoid())\n",
    "        self.generator_nn = nn.Sequential(*generator_layers)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state.to(torch.float)\n",
    "        action = self.generator_nn(state)\n",
    "        return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, we define a function that will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================#\n",
    "#=== TEST FUNCTION ===#\n",
    "#=====================#\n",
    "\n",
    "def test_model(model, model_type, data_name, num_tests, test_data, state_dim, final_epoch):\n",
    "    model.eval()\n",
    "    if final_epoch: print(\"\")\n",
    "\n",
    "    # Track label distributions for the model and for the expert\n",
    "    model_label_distr = np.array([0,0,0,0])\n",
    "    expert_label_distr = np.array([0,0,0,0])\n",
    "\n",
    "    # Determine number of tests\n",
    "    if num_tests == \"all\":\n",
    "        num_tests = len(test_data)\n",
    "    elif num_tests > len(test_data) and len(test_data) != 0:\n",
    "        num_tests = len(test_data)\n",
    "\n",
    "    # Perform test\n",
    "    count_gen = 0\n",
    "    for i in range(num_tests):\n",
    "        print(f\"\\rTesting on {data_name} ({100*(i+1)/num_tests:.1f}%).                   \", end='')\n",
    "        # Get expert action\n",
    "        state_action = test_data[i].to(torch.float)\n",
    "        state = state_action[:state_dim]\n",
    "        expert_label = state_action[state_dim:].to(torch.int).tolist()\n",
    "        # Generate action by model\n",
    "        if model_type == \"nn\":\n",
    "            model_action = model(state)\n",
    "        elif model_type == \"vnn\":\n",
    "            model_action, mu, logvar = model(state)\n",
    "        model_label = to_one_hot(model_action)\n",
    "        \n",
    "        # Count correct generated actions\n",
    "        if expert_label == model_label:\n",
    "            count_gen +=1\n",
    "        \n",
    "        # Update distributions\n",
    "        model_label_distr += np.array(model_label)\n",
    "        expert_label_distr += np.array(expert_label)\n",
    "    \n",
    "    # Calculate score\n",
    "    model_score = 100.*count_gen/num_tests\n",
    "    if final_epoch:\n",
    "        print(f\"\\nAccuracy: {model_score:.1f}%, Expert distr: {expert_label_distr}, Model distr: {model_label_distr}\", end='')\n",
    "\n",
    "    return model_score\n",
    "\n",
    "def to_one_hot(lst):\n",
    "    # Find the index of the maximum value in the list\n",
    "    max_index = max(range(len(lst)), key=lst.__getitem__)\n",
    "    # Create a new list with all zeros except for a 1 at the max index\n",
    "    one_hot = [0] * len(lst)\n",
    "    one_hot[max_index] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the NN model. We choose the number of neurons such that the data is about 10 times the number of weights. We only save the model if it beats the all time best model so far in terms of performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================#\n",
    "#=== NN TRAINING ===#\n",
    "#===================#\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hardware setup\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # Split data\n",
    "    all_data, [train_data, val_data, test_data] = data_split(all_data=encoded_poker_data,\n",
    "                                                           split=[0.75, 0.05, 0.2])\n",
    "    \n",
    "    # Params\n",
    "    name = \"nn_classifier\"\n",
    "    save = True\n",
    "    state_dim = 42\n",
    "    hidden_dims = [64,8]\n",
    "    action_dim = 4\n",
    "    lr = 5e-4\n",
    "    weight_decay = 1e-4\n",
    "    batch_size = 512\n",
    "    epochs = 2\n",
    "    loss_func = nn.MSELoss(reduction='sum')\n",
    "    activ_func = nn.Tanh()\n",
    "    \n",
    "    # Define model\n",
    "    nn_classifier = NN(name=name, \n",
    "                       state_dim=state_dim, \n",
    "                       hidden_dims=hidden_dims, \n",
    "                       action_dim=action_dim,\n",
    "                       loss_func=loss_func,\n",
    "                       activ_func=activ_func)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(nn_classifier.parameters(), \n",
    "                                 lr=lr, \n",
    "                                 weight_decay=weight_decay)\n",
    "    criterion = loss_func.to(device)\n",
    "    \n",
    "    # Print model and data information\n",
    "    total_dimension = state_dim\n",
    "    for i in range(len(nn_classifier.generator_nn)):\n",
    "        if type(nn_classifier.generator_nn[i]) == nn.Linear:\n",
    "            layer_dimension = len(nn_classifier.generator_nn[i].weight)\n",
    "            total_dimension *= layer_dimension\n",
    "    print(f\"Model size: {total_dimension}.\")\n",
    "    print(f\"Data size: {len(train_data)}.\")\n",
    "    print(f\"Data size / Model size: {len(train_data)/total_dimension:.2f}.\\n\")\n",
    "\n",
    "    # Load mordel and data onto device\n",
    "    nn_classifier = nn_classifier.to(device)\n",
    "    train_inputs = torch.tensor(train_data[:, :state_dim])\n",
    "    train_outputs = torch.tensor(train_data[:, state_dim:])\n",
    "    train_dataset = TensorDataset(train_inputs, train_outputs)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_inputs = torch.tensor(val_data[:, :state_dim])\n",
    "    val_outputs = torch.tensor(val_data[:, state_dim:])\n",
    "    val_dataset = TensorDataset(val_inputs, val_outputs)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the network and plot the performance\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    num_epochs = epochs\n",
    "\n",
    "    max_score = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            nn_classifier.train()\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            train_outputs = nn_classifier(inputs.float())\n",
    "            train_loss = criterion(train_outputs, labels.float())\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += train_loss.item()\n",
    "            print(f\"\\rTraining: Epoch {epoch+1}/{epochs} ({100*(i+1)/len(train_dataloader):.1f}%)            \", end='')\n",
    "\n",
    "        # Compute test loss\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            nn_classifier.eval()\n",
    "            for i, data in enumerate(val_dataloader):\n",
    "                inputs, labels = data\n",
    "                val_outputs = nn_classifier(inputs.float())\n",
    "                val_loss += criterion(val_outputs, labels.float()).item()\n",
    "\n",
    "        # Store and print statistics\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Testing\n",
    "        final = True if epoch+1 == num_epochs else False\n",
    "        train_score = test_model(nn_classifier, \"nn\", \"train data\", len(val_data), torch.tensor(train_data), state_dim, final)\n",
    "        val_score = test_model(nn_classifier, \"nn\", \"validation data\", len(val_data), torch.tensor(val_data), state_dim, final)\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "\n",
    "        # Save\n",
    "        if val_score > max_score and save == True:\n",
    "            # Save max score\n",
    "            max_score = val_score\n",
    "            # Save the model\n",
    "            torch.save(nn_classifier.state_dict(), f'models/{nn_classifier.name}.pth')\n",
    "\n",
    "    # Plot performance\n",
    "    epochs = range(num_epochs)\n",
    "\n",
    "    plt.suptitle(\"NN Classifier Performance\", fontweight='bold')\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_scores, label='Train accuracy')\n",
    "    plt.plot(epochs, val_scores, label='Validation accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.savefig('plots/nn_classifier.png')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The variational neural network (vNN) classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "Defining the variational neural network (vNN) model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================#\n",
    "#=== VARIATIONAL NEURAL NETWORK ===#\n",
    "#==================================#\n",
    "\n",
    "class vNN(nn.Module):\n",
    "    def __init__(self, name, state_dim, hidden_dims, action_dim, \n",
    "                 loss_func, activ_func):\n",
    "        super(vNN, self).__init__()\n",
    "        # store input params:\n",
    "        self.name = name\n",
    "        self.input_dim = state_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = action_dim\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        # build generator neural net:\n",
    "        generator_layers = []\n",
    "        prev_dim = state_dim\n",
    "        for dim in hidden_dims:\n",
    "            generator_layers.append(nn.Linear(prev_dim, dim))\n",
    "            generator_layers.append(activ_func)\n",
    "            prev_dim = dim\n",
    "        generator_layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        generator_layers.append(nn.Sigmoid())\n",
    "        self.generator_nn = nn.Sequential(*generator_layers)\n",
    "        self.fc31 = nn.Linear(action_dim, action_dim) # mean\n",
    "        self.fc32 = nn.Linear(action_dim, action_dim) # log variance\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state.to(torch.float)\n",
    "        action = self.generator_nn(state)\n",
    "        mu = self.fc31(action)\n",
    "        logvar = self.fc32(action)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We train the vNN model. We choose the number of neurons such that the data is about 10 times the number of weights. We only save the model if it beats the all time best model so far in terms of performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================#\n",
    "#=== vNN TRAINING ===#\n",
    "#====================#\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hardware setup\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # Split data\n",
    "    all_data, [train_data, val_data, test_data] = data_split(all_data=encoded_poker_data,\n",
    "                                                           split=[0.75, 0.05, 0.2])\n",
    "    \n",
    "    # Params\n",
    "    name = \"vnn_classifier\"\n",
    "    save = True\n",
    "    state_dim = 42\n",
    "    hidden_dims = [64,8]\n",
    "    action_dim = 4\n",
    "    lr = 5e-4\n",
    "    weight_decay = 1e-4\n",
    "    batch_size = 512\n",
    "    epochs = 50\n",
    "    loss_func = nn.MSELoss(reduction='sum')\n",
    "    activ_func = nn.Tanh()\n",
    "    \n",
    "    # Define model\n",
    "    vnn_classifier = vNN(name=name, \n",
    "                        state_dim=state_dim, \n",
    "                        hidden_dims=hidden_dims, \n",
    "                        action_dim=action_dim,\n",
    "                        loss_func=loss_func,\n",
    "                        activ_func=activ_func)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(vnn_classifier.parameters(), \n",
    "                                 lr=lr, \n",
    "                                 weight_decay=weight_decay)\n",
    "    criterion = loss_func.to(device)\n",
    "    \n",
    "    # Print model and data information\n",
    "    total_dimension = state_dim\n",
    "    for i in range(len(vnn_classifier.generator_nn)):\n",
    "        if type(vnn_classifier.generator_nn[i]) == nn.Linear:\n",
    "            layer_dimension = len(vnn_classifier.generator_nn[i].weight)\n",
    "            total_dimension *= layer_dimension\n",
    "    print(f\"Model size: {total_dimension}.\")\n",
    "    print(f\"Data size: {len(train_data)}.\")\n",
    "    print(f\"Data size / Model size: {len(train_data)/total_dimension:.2f}.\\n\")\n",
    "\n",
    "    # Load mordel and data onto device\n",
    "    vnn_classifier = vnn_classifier.to(device)\n",
    "    train_inputs = torch.tensor(train_data[:, :state_dim])\n",
    "    train_outputs = torch.tensor(train_data[:, state_dim:])\n",
    "    train_dataset = TensorDataset(train_inputs, train_outputs)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_inputs = torch.tensor(val_data[:, :state_dim])\n",
    "    val_outputs = torch.tensor(val_data[:, state_dim:])\n",
    "    val_dataset = TensorDataset(val_inputs, val_outputs)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the network and plot the performance\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    num_epochs = epochs\n",
    "\n",
    "    max_score = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            vnn_classifier.train()\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            train_outputs, mu, logvar = vnn_classifier(inputs.float())\n",
    "            train_loss = criterion(train_outputs, labels.float())\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += train_loss.item()\n",
    "            print(f\"\\rTraining: Epoch {epoch+1}/{epochs} ({100*(i+1)/len(train_dataloader):.1f}%)            \", end='')\n",
    "\n",
    "        # Compute test loss\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            vnn_classifier.eval()\n",
    "            for i, data in enumerate(val_dataloader):\n",
    "                inputs, labels = data\n",
    "                val_outputs, _, _ = vnn_classifier(inputs.float())\n",
    "                val_loss += criterion(val_outputs, labels.float()).item()\n",
    "\n",
    "        # Store and print statistics\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Testing\n",
    "        final = True if epoch+1 == num_epochs else False\n",
    "        train_score = test_model(vnn_classifier, \"vnn\", \"train data\", len(val_data), torch.tensor(train_data), state_dim, final)\n",
    "        val_score = test_model(vnn_classifier, \"vnn\", \"validation data\", len(val_data), torch.tensor(val_data), state_dim, final)\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "\n",
    "        # Save\n",
    "        if val_score > max_score and save == True:\n",
    "            # Save max score\n",
    "            max_score = val_score\n",
    "            # Save the model\n",
    "            torch.save(vnn_classifier.state_dict(), f'models/{vnn_classifier.name}.pth')\n",
    "\n",
    "    # Plot performance\n",
    "    epochs = range(num_epochs)\n",
    "\n",
    "    plt.suptitle(\"vNN Classifier Performance\", fontweight='bold')\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_scores, label='Train accuracy')\n",
    "    plt.plot(epochs, val_scores, label='Validation accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.savefig('plots/vnn_classifier.png')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "We will now test the error and accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the data\n",
    "test_inputs = torch.tensor(test_data[:, :state_dim])\n",
    "test_outputs = torch.tensor(test_data[:, state_dim:])\n",
    "test_dataset = TensorDataset(test_inputs, test_outputs)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best saved models\n",
    "nn_classifier.load_state_dict(torch.load(\"models/nn_classifier.pth\"))\n",
    "vnn_classifier.load_state_dict(torch.load(\"models/vnn_classifier.pth\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of the NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test loss\n",
    "nn_test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    nn_classifier.eval()\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        test_outputs = nn_classifier(inputs.float())\n",
    "        nn_test_loss += criterion(test_outputs, labels.float()).item()\n",
    "\n",
    "print(f\"NN model: test loss = {nn_test_loss:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test accuracy\n",
    "nn_test_score = test_model(nn_classifier, \"nn\", \"test data\", len(test_data), torch.tensor(test_data), state_dim)\n",
    "print(f\"\\nNN model: test accuracy = {nn_test_score:.1f}%.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of the vNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test score\n",
    "vnn_test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    vnn_classifier.eval()\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        test_outputs, _, _ = vnn_classifier(inputs.float())\n",
    "        vnn_test_loss += criterion(test_outputs, labels.float()).item()\n",
    "\n",
    "print(f\"vNN model: test loss = {vnn_test_loss:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test accuracy\n",
    "vnn_test_score = test_model(vnn_classifier, \"vnn\", \"test data\", len(test_data), torch.tensor(test_data), state_dim)\n",
    "print(f\"\\nvNN model: test accuracy = {vnn_test_score}%.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Our goal was to find out how well a NN classifier and a vNN classifier can learn a stochastic dataset such as a poker dataset, and which model is best. Based on the results above, the NN classifier is superior to the vNN classifier. The NN classifier learns faster, reaches lower errors, and reaches higher accuracies on training, validation, and test datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
